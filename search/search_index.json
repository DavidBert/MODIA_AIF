{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Artificial Intelligence Frameworks Hello and welcome to the AI Frameworks 2024 course! This course serves as a successor to the Machine Learning and High Dimensional & Deep Learning courses. While the aforementioned courses delved into the theory and methods of machine learning and deep learning, our current course is designed to equip you with the practical skills necessary to apply these methods in real-world scenarios. Specifically, we'll explore the following areas: Development Environment and Tools Python scripting Version control using Git and GitHub Working with Docker containers Ensuring reproducibility Session 2: Recommender Systems Collaborative filtering Content-based filtering Hybrid recommender systems Evaluating recommender systems Session 3: Natural Language Processing (NLP) Text preprocessing Bag of Words approach Word embeddings Introduction to Transformers Session 4: Reinforcement Learning Understanding Markov decision processes Policy evaluation and improvement Introduction to Q-learning and Deep Q-learning Session 5: Explainable AI (XAI) Understanding feature importance Post-hoc explainability Delving into local and global explainability Exploring black-box explainability Introduction to attribution methods Each session consists of three components: Theoretical Learning: This includes markdown documents and videos. Slides accompanying the videos will be provided for every lesson. Please review these materials before the practical sessions. Weekly reminders and content updates will be sent out. Should you have any queries, don't hesitate to raise them during the practical sessions or email me at david.bertoin (at) insa-toulouse.fr. You will be evaluated on your understanding of the theoretical concepts through quizzes at the beginning of each practical session. Practical Application: Comprising a series of Jupyter notebooks and Python scripts, this hands-on component requires you to code and answer questions. A teaching assistant will be available to assist and clarify any doubts. Additionally, you're not expected to complete this in a single session; feel free to continue at home. This is also the time to work on your final project. Final Project: In teams of 4-5, you'll harness the knowledge gained throughout the course. The project encompasses two major tasks: constructing a movie recommender system with a user interface, and crafting a reinforcement learning agent to navigate a video game, while elucidating its decisions using XAI tools. While an hour is dedicated at the end of each practical session for this, additional work at home is anticipated. Upon course completion, you'll be expected to present a Docker image of your project along with its GitHub repository. Further specifics will be shared during the practical sessions. Evaluation Grading will be based on the final project (60%) and a quizzes (40%). The quizzes will be held at the beginning of each session and will test your understanding of the theoretical concepts. Knowledge requirements Python Tutorial Machine Learning High Dimensional & Deep Learning","title":"Home"},{"location":"index.html#artificial-intelligence-frameworks","text":"Hello and welcome to the AI Frameworks 2024 course! This course serves as a successor to the Machine Learning and High Dimensional & Deep Learning courses. While the aforementioned courses delved into the theory and methods of machine learning and deep learning, our current course is designed to equip you with the practical skills necessary to apply these methods in real-world scenarios. Specifically, we'll explore the following areas: Development Environment and Tools Python scripting Version control using Git and GitHub Working with Docker containers Ensuring reproducibility Session 2: Recommender Systems Collaborative filtering Content-based filtering Hybrid recommender systems Evaluating recommender systems Session 3: Natural Language Processing (NLP) Text preprocessing Bag of Words approach Word embeddings Introduction to Transformers Session 4: Reinforcement Learning Understanding Markov decision processes Policy evaluation and improvement Introduction to Q-learning and Deep Q-learning Session 5: Explainable AI (XAI) Understanding feature importance Post-hoc explainability Delving into local and global explainability Exploring black-box explainability Introduction to attribution methods Each session consists of three components:","title":"Artificial Intelligence Frameworks"},{"location":"index.html#theoretical-learning","text":"This includes markdown documents and videos. Slides accompanying the videos will be provided for every lesson. Please review these materials before the practical sessions. Weekly reminders and content updates will be sent out. Should you have any queries, don't hesitate to raise them during the practical sessions or email me at david.bertoin (at) insa-toulouse.fr. You will be evaluated on your understanding of the theoretical concepts through quizzes at the beginning of each practical session.","title":"Theoretical Learning:"},{"location":"index.html#practical-application","text":"Comprising a series of Jupyter notebooks and Python scripts, this hands-on component requires you to code and answer questions. A teaching assistant will be available to assist and clarify any doubts. Additionally, you're not expected to complete this in a single session; feel free to continue at home. This is also the time to work on your final project.","title":"Practical Application:"},{"location":"index.html#final-project","text":"In teams of 4-5, you'll harness the knowledge gained throughout the course. The project encompasses two major tasks: constructing a movie recommender system with a user interface, and crafting a reinforcement learning agent to navigate a video game, while elucidating its decisions using XAI tools. While an hour is dedicated at the end of each practical session for this, additional work at home is anticipated. Upon course completion, you'll be expected to present a Docker image of your project along with its GitHub repository. Further specifics will be shared during the practical sessions.","title":"Final Project:"},{"location":"index.html#evaluation","text":"Grading will be based on the final project (60%) and a quizzes (40%). The quizzes will be held at the beginning of each session and will test your understanding of the theoretical concepts.","title":"Evaluation"},{"location":"index.html#knowledge-requirements","text":"Python Tutorial Machine Learning High Dimensional & Deep Learning","title":"Knowledge requirements"},{"location":"schedule.html","text":"Schedule Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures. Session 0 Course Introduction Session 1: Developpement tools 09/10/2023 (9h30-12h15 & 13h30-16h15) Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2 Session 2: Introduction to recommender systems 13/11/2023 (9h30-12h15 & 13h30-16h15) Session 3: Introduction to Natural language processing 27/11/2023 (9h30-12h15 & 13h30-16h15) Session 4: Introduction to Reinforcement Learning 18/12/2023 (9h30-12h15 & 13h30-16h15) Session 5: Explainability methods 08/01/2024 (9h30-12h15 & 13h30-16h15)","title":"Schedule"},{"location":"schedule.html#schedule","text":"Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures.","title":"Schedule"},{"location":"schedule.html#session-0","text":"Course Introduction","title":"Session 0"},{"location":"schedule.html#session-1-developpement-tools","text":"","title":"Session 1: Developpement tools"},{"location":"schedule.html#09102023-9h30-12h15-13h30-16h15","text":"Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2","title":"09/10/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-2-introduction-to-recommender-systems","text":"","title":"Session 2: Introduction to recommender systems"},{"location":"schedule.html#13112023-9h30-12h15-13h30-16h15","text":"","title":"13/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-3-introduction-to-natural-language-processing","text":"","title":"Session 3: Introduction to Natural language processing"},{"location":"schedule.html#27112023-9h30-12h15-13h30-16h15","text":"","title":"27/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-4-introduction-to-reinforcement-learning","text":"","title":"Session 4: Introduction to Reinforcement Learning"},{"location":"schedule.html#18122023-9h30-12h15-13h30-16h15","text":"","title":"18/12/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"schedule.html#session-5-explainability-methods","text":"","title":"Session 5: Explainability methods"},{"location":"schedule.html#08012024-9h30-12h15-13h30-16h15","text":"","title":"08/01/2024 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"developpement/api.html","text":"Development for Data Scientist: Deploying a Machine Learning Model using a REST API in Python In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python . What is Model Deployment? Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return. Why is Model Deployment Crucial? Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use. Challenges in Model Deployment: Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services. What is an API? An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality. Key Concepts in API: Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models. Overview of RESTful Services: REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods . Principles of REST: Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently. Components of REST APIs: Endpoints: An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions. HTTP Methods: RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource. Status Codes: HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered. Requests & Responses: Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models. Interacting with REST APIs using Python Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise. Crafting a GET Request for Weather Data We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details. Sending Data with POST Requests in Python While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask. Building a Simple Flask API Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/. Requesting the Flask API using Python With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Introduction to REST APIs"},{"location":"developpement/api.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/api.html#deploying-a-machine-learning-model-using-a-rest-api-in-python","text":"In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python .","title":"Deploying a Machine Learning Model using a REST API in Python"},{"location":"developpement/api.html#what-is-model-deployment","text":"Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return.","title":"What is Model Deployment?"},{"location":"developpement/api.html#why-is-model-deployment-crucial","text":"Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use.","title":"Why is Model Deployment Crucial?"},{"location":"developpement/api.html#challenges-in-model-deployment","text":"Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services.","title":"Challenges in Model Deployment:"},{"location":"developpement/api.html#what-is-an-api","text":"An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality.","title":"What is an API?"},{"location":"developpement/api.html#key-concepts-in-api","text":"Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models.","title":"Key Concepts in API:"},{"location":"developpement/api.html#overview-of-restful-services","text":"REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods .","title":"Overview of RESTful Services:"},{"location":"developpement/api.html#principles-of-rest","text":"Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently.","title":"Principles of REST:"},{"location":"developpement/api.html#components-of-rest-apis","text":"","title":"Components of REST APIs:"},{"location":"developpement/api.html#endpoints","text":"An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions.","title":"Endpoints:"},{"location":"developpement/api.html#http-methods","text":"RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource.","title":"HTTP Methods:"},{"location":"developpement/api.html#status-codes","text":"HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered.","title":"Status Codes:"},{"location":"developpement/api.html#requests-responses","text":"Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models.","title":"Requests &amp; Responses:"},{"location":"developpement/api.html#interacting-with-rest-apis-using-python","text":"Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise.","title":"Interacting with REST APIs using Python"},{"location":"developpement/api.html#crafting-a-get-request-for-weather-data","text":"We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details.","title":"Crafting a GET Request for Weather Data"},{"location":"developpement/api.html#sending-data-with-post-requests-in-python","text":"While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask.","title":"Sending Data with POST Requests in Python"},{"location":"developpement/api.html#building-a-simple-flask-api","text":"Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/.","title":"Building a Simple Flask API"},{"location":"developpement/api.html#requesting-the-flask-api-using-python","text":"With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Requesting the Flask API using Python"},{"location":"developpement/colorize.html","text":"Development for Data Scientist: Practical session 2: Deploying a digit classifier Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh Data We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version. The network architecture We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine. API, Web app and deployment Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE. Solutions: model.py train.py colorize_api.py colorize_web_app.py","title":"Practical session 2"},{"location":"developpement/colorize.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/colorize.html#practical-session-2-deploying-a-digit-classifier","text":"Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it.","title":"Practical session 2: Deploying a digit classifier"},{"location":"developpement/colorize.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh","title":"Practical session repository:"},{"location":"developpement/colorize.html#data","text":"We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version.","title":"Data"},{"location":"developpement/colorize.html#the-network-architecture","text":"We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py","title":"The network architecture"},{"location":"developpement/colorize.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine.","title":"Training script"},{"location":"developpement/colorize.html#api-web-app-and-deployment","text":"Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE.","title":"API, Web app and deployment"},{"location":"developpement/colorize.html#solutions","text":"","title":"Solutions:"},{"location":"developpement/colorize.html#modelpy","text":"","title":"model.py"},{"location":"developpement/colorize.html#trainpy","text":"","title":"train.py"},{"location":"developpement/colorize.html#colorize_apipy","text":"","title":"colorize_api.py"},{"location":"developpement/colorize.html#colorize_web_apppy","text":"","title":"colorize_web_app.py"},{"location":"developpement/docker.html","text":"Development for Data Scientist: Docker Course (Video by Brendan Guillouet) Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard RUN pip install markupsafe==2.0.1 Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] [path_to_your_dockerfile] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[absolute_path_to_your_folder_to_share]:/workspace/[folder_name_in_the_container] [image_name] for instance: docker run -it --name my_container_name -v ~/workspace/colorize:/workspace/colorize_container my_image_name Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder on your local machine. What can you see? Now try to run your applications on your cloud instance. Send the Dockerfile and the folder containing your applications to your cloud instance. On the cloud instance, build your image and run your container and your app in background mode. sudo docker exec -t my_container_name python ./colorize_container/colorize_app.py --weights_path ./colorize_container/unet.pth That's it! You have deployed a machine learning application on a cloud machine it is now accessible from everywhere. Send the url to one of your classmate and ask him/her to test your app. This is it for this session. Please remember to shutdown your cloud machine with the command: sudo shutdown -h now Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Introduction to Docker"},{"location":"developpement/docker.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/docker.html#docker","text":"","title":"Docker"},{"location":"developpement/docker.html#course-video-by-brendan-guillouet","text":"Slides","title":"Course (Video by Brendan Guillouet)"},{"location":"developpement/docker.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: Improving the reproducibility of the results Facilitating the portability and deployment In this session, we will try to package the code from our Gradio applications, allowing us to predict digits labels and to colorize images into a Docker image. We will then use this image to instantiate a container that could be hosted on any physical device to run the app. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install gradio tensorboard RUN pip install markupsafe==2.0.1 Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here . If docker is not already installed in your machine, follow this guide to install it. You may now build your first image using the following command: sudo docker build -t [your_image_name] [path_to_your_dockerfile] The image should take a few minutes to build. Once it is done, use the following command to list the available images on your device: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal within your container. On this terminal, open a Python console and check that Pytorch is installed. import torch print(torch.__version__) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: colorize_app.py mnist_app.py mnist.pth unet.pth Create a new container, this time mounting a shared volume with the following command: docker run -it --name [container_name] -v ~/[absolute_path_to_your_folder_to_share]:/workspace/[folder_name_in_the_container] [image_name] for instance: docker run -it --name my_container_name -v ~/workspace/colorize:/workspace/colorize_container my_image_name Try to run one of your Gradio applications using the interactive mode. cd [folder_name] python colorize_app.py Leave the container and look at your folder on your local machine. What can you see? Now try to run your applications on your cloud instance. Send the Dockerfile and the folder containing your applications to your cloud instance. On the cloud instance, build your image and run your container and your app in background mode. sudo docker exec -t my_container_name python ./colorize_container/colorize_app.py --weights_path ./colorize_container/unet.pth That's it! You have deployed a machine learning application on a cloud machine it is now accessible from everywhere. Send the url to one of your classmate and ask him/her to test your app. This is it for this session. Please remember to shutdown your cloud machine with the command: sudo shutdown -h now Do not hesitate to play a little more with Docker. For instance try to train the MNIST classifier directly in your container and to collect the tensorboard logs and the resulting weights on your local machine.","title":"Practical Session"},{"location":"developpement/git_intro.html","text":"Introduction to git Slides","title":"Introduction to Git"},{"location":"developpement/git_intro.html#introduction-to-git","text":"Slides","title":"Introduction to git"},{"location":"developpement/mnist.html","text":"Development for Data Scientist: Practical session 1: Deploying a digit classifier For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. The network class: Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x The training script The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64 Monitoring and experiment management Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network Deploying your model with Flask: Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5000 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model. Deploying your model with Gradio As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_webapp.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST? Git Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository. Docker Dockers are a way to package your application and all its dependencies in a single image that can be run on any machine. The file Dockerfile contains the instructions to build a docker image that will run your application. Build the image with the following command: sudo docker build -t mnist-flask-app . This will create a docker image named mnist-flask-app . A docker image is a read-only template that contains a set of instructions for creating a container that can run on the Docker platform. You can list all the images on your machine with the following command: sudo docker images A docker container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. Run the container with the following command: sudo docker run -p 5000:5000 mnist-flask-app You can now access your application by going to http://localhost:5000 in your browser. By defult the container will run the command python mnist_app.py --weights_path weights/mnist_net.pth when it starts. That's it! You have created a docker image that can be run on any machine that has docker installed. By doing so, you have created a reproducible environment for your application that can be run on any machine. This is very useful when you want to deploy your application on a server or in the cloud. To list all the running containers use the following command: sudo docker ps To list all the containers (running and stopped) use the following command: sudo docker ps -a To stop a container use the following command: sudo docker stop [CONTAINER_ID] To remove a container use the following command: sudo docker rm [CONTAINER_ID] To remove an image use the following command: sudo docker rmi [IMAGE_ID] DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE. Solutions: model.py train.py mnist_api.py mnist_webapp.py","title":"Practical session 1"},{"location":"developpement/mnist.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/mnist.html#practical-session-1-deploying-a-digit-classifier","text":"For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script.","title":"Practical session 1: Deploying a digit classifier"},{"location":"developpement/mnist.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"developpement/mnist.html#the-network-class","text":"Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x","title":"The network class:"},{"location":"developpement/mnist.html#the-training-script","text":"The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64","title":"The training script"},{"location":"developpement/mnist.html#monitoring-and-experiment-management","text":"Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network","title":"Monitoring and experiment management"},{"location":"developpement/mnist.html#deploying-your-model-with-flask","text":"Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5000 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model.","title":"Deploying your model with Flask:"},{"location":"developpement/mnist.html#deploying-your-model-with-gradio","text":"As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_webapp.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST?","title":"Deploying your model with Gradio"},{"location":"developpement/mnist.html#git","text":"Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Git"},{"location":"developpement/mnist.html#docker","text":"Dockers are a way to package your application and all its dependencies in a single image that can be run on any machine. The file Dockerfile contains the instructions to build a docker image that will run your application. Build the image with the following command: sudo docker build -t mnist-flask-app . This will create a docker image named mnist-flask-app . A docker image is a read-only template that contains a set of instructions for creating a container that can run on the Docker platform. You can list all the images on your machine with the following command: sudo docker images A docker container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. Run the container with the following command: sudo docker run -p 5000:5000 mnist-flask-app You can now access your application by going to http://localhost:5000 in your browser. By defult the container will run the command python mnist_app.py --weights_path weights/mnist_net.pth when it starts. That's it! You have created a docker image that can be run on any machine that has docker installed. By doing so, you have created a reproducible environment for your application that can be run on any machine. This is very useful when you want to deploy your application on a server or in the cloud. To list all the running containers use the following command: sudo docker ps To list all the containers (running and stopped) use the following command: sudo docker ps -a To stop a container use the following command: sudo docker stop [CONTAINER_ID] To remove a container use the following command: sudo docker rm [CONTAINER_ID] To remove an image use the following command: sudo docker rmi [IMAGE_ID] DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE.","title":"Docker"},{"location":"developpement/mnist.html#solutions","text":"","title":"Solutions:"},{"location":"developpement/mnist.html#modelpy","text":"","title":"model.py"},{"location":"developpement/mnist.html#trainpy","text":"","title":"train.py"},{"location":"developpement/mnist.html#mnist_apipy","text":"","title":"mnist_api.py"},{"location":"developpement/mnist.html#mnist_webapppy","text":"","title":"mnist_webapp.py"},{"location":"developpement/pytorch.html","text":"Development for Data Scientist: Pytorch and Python Script Course Course notebook: Notebook","title":"Introduction to Pytorch"},{"location":"developpement/pytorch.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"developpement/pytorch.html#pytorch-and-python-script","text":"","title":"Pytorch and Python Script"},{"location":"developpement/pytorch.html#course","text":"","title":"Course"},{"location":"developpement/pytorch.html#course-notebook","text":"Notebook","title":"Course notebook:"},{"location":"developpement/quizz.html","text":"Quizz: Developpement tools for Data Scientist Chargement\u2026","title":"Quizz"},{"location":"developpement/quizz.html#quizz-developpement-tools-for-data-scientist","text":"Chargement\u2026","title":"Quizz: Developpement tools for Data Scientist"},{"location":"docs/evaluation.html","text":"Evaluation The evaluation is associated to the DEFI-IA (Introduction video link ) Objectives You will be evaluated on your capacity of acting like a Data Scientist , i.e. Collect the data. Doing some exploratory analysis. Create new features. Write a complete pipeline to train and test your models. Justify your modelisation choices. Interpret your results. Work in group (Git). Share it and make your results easily reproducible (Docker, Gradio). Evaluation criteria You are expected to produce a code that is easily readable and reproducible. Your code should at leat contain the three following files (but you are ecouraged to add more to make it more readable): * train.py : the training script * app.py : code to launch a gradio application to test your model (see Gradio ) * analysis.ipynb : a notebook containing your exploratory analysis and interpretability results on your model. * Dockerfile : a Dockerfile to build a docker image of your application (see Docker ) You will be evaluated on the following criteria: Project - ( 70% ): You must provide a git repository with a complete history of your commits. Your capacity to work in group will be evaluated, your commit history must contain commits from several users at different dates. You must provide a Dockerfile to build a docker image that can be used to run your code (training and the Gradio application). The git should contain a clear markdown Readme, which describes: the result you achieved the commands to run for training your model or launching the gradio application (from a docker container) The code should be clear and easily readable. No notebooks exept for the exploratory analysis. * Oral presentation - ( 30% ) 15 minutes presentation + 10 minutes questions. You will be evaluated on the following criteria: Quality of the presentation. Explanations of the chosen features and algorithm. Demonstration of your application. Some insights on your model biais and interpretability. Other details Group of 4 people (DEFI IA's team).","title":"Evaluation"},{"location":"docs/evaluation.html#evaluation","text":"The evaluation is associated to the DEFI-IA (Introduction video link )","title":"Evaluation"},{"location":"docs/evaluation.html#objectives","text":"You will be evaluated on your capacity of acting like a Data Scientist , i.e. Collect the data. Doing some exploratory analysis. Create new features. Write a complete pipeline to train and test your models. Justify your modelisation choices. Interpret your results. Work in group (Git). Share it and make your results easily reproducible (Docker, Gradio).","title":"Objectives"},{"location":"docs/evaluation.html#evaluation-criteria","text":"You are expected to produce a code that is easily readable and reproducible. Your code should at leat contain the three following files (but you are ecouraged to add more to make it more readable): * train.py : the training script * app.py : code to launch a gradio application to test your model (see Gradio ) * analysis.ipynb : a notebook containing your exploratory analysis and interpretability results on your model. * Dockerfile : a Dockerfile to build a docker image of your application (see Docker ) You will be evaluated on the following criteria: Project - ( 70% ): You must provide a git repository with a complete history of your commits. Your capacity to work in group will be evaluated, your commit history must contain commits from several users at different dates. You must provide a Dockerfile to build a docker image that can be used to run your code (training and the Gradio application). The git should contain a clear markdown Readme, which describes: the result you achieved the commands to run for training your model or launching the gradio application (from a docker container) The code should be clear and easily readable. No notebooks exept for the exploratory analysis. * Oral presentation - ( 30% ) 15 minutes presentation + 10 minutes questions. You will be evaluated on the following criteria: Quality of the presentation. Explanations of the chosen features and algorithm. Demonstration of your application. Some insights on your model biais and interpretability.","title":"Evaluation criteria"},{"location":"docs/evaluation.html#other-details","text":"Group of 4 people (DEFI IA's team).","title":"Other details"},{"location":"docs/introduction.html","text":"Course Introduction: Hi, welcome to the AI frameworks 2024 course.","title":"Course Introduction:"},{"location":"docs/introduction.html#course-introduction","text":"Hi, welcome to the AI frameworks 2024 course.","title":"Course Introduction:"},{"location":"docs/schedule.html","text":"Schedule Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures. Session 0 Course Introduction Session 1: Developpement tools 09/10/2023 (9h30-12h15 & 13h30-16h15) Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2 Session 2: Introduction to recommender systems 13/11/2023 (9h30-12h15 & 13h30-16h15) Session 3: Introduction to Natural language processing 27/11/2023 (9h30-12h15 & 13h30-16h15) Session 4: Introduction to Reinforcement Learning 18/12/2023 (9h30-12h15 & 13h30-16h15) Session 5: Explainability methods 08/01/2024 (9h30-12h15 & 13h30-16h15)","title":"Schedule"},{"location":"docs/schedule.html#schedule","text":"Lectures : 10 hours Practical Sessions : 30 hours. Fives days are dedicated to the practical sessions. All the lectures associated to each sessions are available in video. You must have seen the corresponding videos before each session. At the start of each practical session, approximately 15 minutes will be devoted to questions about the lectures.","title":"Schedule"},{"location":"docs/schedule.html#session-0","text":"Course Introduction","title":"Session 0"},{"location":"docs/schedule.html#session-1-developpement-tools","text":"","title":"Session 1: Developpement tools"},{"location":"docs/schedule.html#09102023-9h30-12h15-13h30-16h15","text":"Introduction to Pytorch and Python scripts Github Reminder Introduction to Docker Practical session 1 Practical session 2","title":"09/10/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"docs/schedule.html#session-2-introduction-to-recommender-systems","text":"","title":"Session 2: Introduction to recommender systems"},{"location":"docs/schedule.html#13112023-9h30-12h15-13h30-16h15","text":"","title":"13/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"docs/schedule.html#session-3-introduction-to-natural-language-processing","text":"","title":"Session 3: Introduction to Natural language processing"},{"location":"docs/schedule.html#27112023-9h30-12h15-13h30-16h15","text":"","title":"27/11/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"docs/schedule.html#session-4-introduction-to-reinforcement-learning","text":"","title":"Session 4: Introduction to Reinforcement Learning"},{"location":"docs/schedule.html#18122023-9h30-12h15-13h30-16h15","text":"","title":"18/12/2023 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"docs/schedule.html#session-5-explainability-methods","text":"","title":"Session 5: Explainability methods"},{"location":"docs/schedule.html#08012024-9h30-12h15-13h30-16h15","text":"","title":"08/01/2024 (9h30-12h15 &amp; 13h30-16h15)"},{"location":"docs/developpement/api.html","text":"Development for Data Scientist: Deploying a Machine Learning Model using a REST API in Python In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python . What is Model Deployment? Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return. Why is Model Deployment Crucial? Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use. Challenges in Model Deployment: Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services. What is an API? An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality. Key Concepts in API: Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models. Overview of RESTful Services: REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods . Principles of REST: Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently. Components of REST APIs: Endpoints: An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions. HTTP Methods: RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource. Status Codes: HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered. Requests & Responses: Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models. Interacting with REST APIs using Python Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise. Crafting a GET Request for Weather Data We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details. Sending Data with POST Requests in Python While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask. Building a Simple Flask API Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/. Requesting the Flask API using Python With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Development for Data Scientist:"},{"location":"docs/developpement/api.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/developpement/api.html#deploying-a-machine-learning-model-using-a-rest-api-in-python","text":"In this lesson, we'll learn about REST APIs and how to deploy a machine learning model using Flask in Python .","title":"Deploying a Machine Learning Model using a REST API in Python"},{"location":"docs/developpement/api.html#what-is-model-deployment","text":"Model deployment refers to the process of integrating a trained machine learning or statistical model into a production environment. The goal is to make the model's capabilities available to end-users, applications, or services. In the context of data science, deploying a model often means offering it as a service where applications send data to the model and receive predictions in return.","title":"What is Model Deployment?"},{"location":"docs/developpement/api.html#why-is-model-deployment-crucial","text":"Jupyter notebook or a local environment doesn't add business value. To realize its potential value, it must be made available where it's needed. Model deployment is the bridge between building a model and getting it into the hands of users. It's the last step in the data science pipeline but often one of the most complex. Without effective deployment, even the most sophisticated models are of little use.","title":"Why is Model Deployment Crucial?"},{"location":"docs/developpement/api.html#challenges-in-model-deployment","text":"Model deployment is a complex process that involves several challenges. Some of the most common ones are: Scalability : Models may need to handle thousands of requests per second in some applications. It's crucial to ensure that deployment solutions can scale as needed. Latency : Especially in real-time applications, the time taken for a model to provide a prediction (latency) can be critical. Version Control : Models evolve over time. Deployment strategies need to account for versioning to ensure consistent and reproducible results. Dependency Management : Models might rely on specific libraries and versions, making dependency management a significant concern during deployment. While understanding the importance and intricacies of model deployment is crucial, the practical aspect involves choosing a suitable method for deployment. One of the most popular and effective ways to deploy our models is by using REST APIs . REST (Representational State Transfer) APIs provide a standardized way to make our models accessible over the internet. This method not only makes it easier to integrate our models into different applications but also offers scalability and flexibility. With REST APIs, we can encapsulate our models as services that can be consumed by various applications, be it web, mobile, or other services.","title":"Challenges in Model Deployment:"},{"location":"docs/developpement/api.html#what-is-an-api","text":"An API, or Application Programming Interface, is a set of rules and protocols that allow different software entities to communicate with each other. It specifies how software components should interact, enabling the integration of different systems and sharing of data and functionality.","title":"What is an API?"},{"location":"docs/developpement/api.html#key-concepts-in-api","text":"Endpoint : A specific address or URL where an API can be accessed. Request & Response : The essential interaction in an API involves sending a request and receiving a response. Rate Limiting : Restrictions on how many API calls a user or system can make within a specified time frame. While various types of APIs exist, such as SOAP , GraphQL , and RPC , we'll be concentrating on REST APIs in this course due to their ubiquity and relevance in deploying machine learning models.","title":"Key Concepts in API:"},{"location":"docs/developpement/api.html#overview-of-restful-services","text":"REST , or Representational State Transfer, is an architectural style for designing networked applications. RESTful services or APIs are designed around the principles of REST and use standard HTTP methods .","title":"Overview of RESTful Services:"},{"location":"docs/developpement/api.html#principles-of-rest","text":"Statelessness : Each request from a client contains all the information needed to process the request. The server retains no session information. Client-Server Architecture : REST APIs follow a client-server model where the client is responsible for the user interface and user experience, and the server manages the data. Cacheability : Responses from the server can be cached on the client side to improve performance. Uniform Interface : A consistent and unified interface simplifies and decouples the architecture. Deploying a machine learning model using REST APIs is a common practice in the industry. It allows us to encapsulate our model as a service that can be consumed by other applications. This approach offers several advantages: Platform Independence : RESTful APIs can be consumed by any client that understands HTTP, regardless of the platform or language. Scalability : RESTful services are stateless, making it easier to scale by simply adding more servers. Performance : With the ability to cache responses, REST APIs can reduce the number of requests, improving the performance. Flexibility : Data can be returned in multiple formats, such as JSON or XML, allowing for flexibility in how it's consumed. APIs, especially RESTful APIs, are essential tools in the world of software integration. In the context of deploying data science models, they provide a mechanism to make the model accessible to other systems and applications in a standardized manner. As we delve deeper into this course, we'll see how to harness the power of REST APIs to deploy and serve our machine learning models efficiently.","title":"Principles of REST:"},{"location":"docs/developpement/api.html#components-of-rest-apis","text":"","title":"Components of REST APIs:"},{"location":"docs/developpement/api.html#endpoints","text":"An endpoint refers to a specific address (URL) in an API where particular functions can be accessed. For example, /predict might be an endpoint for a machine learning model where you send data for predictions. For example, https://api.example.com/predict could be an endpoint for a machine learning model where you send data for predictions.","title":"Endpoints:"},{"location":"docs/developpement/api.html#http-methods","text":"RESTful APIs use standard HTTP methods to denote the type of action to be performed: - GET : Retrieve data. - POST : Send data for processing or create a new resource. - PUT : Update an existing resource. - DELETE : Remove a resource.","title":"HTTP Methods:"},{"location":"docs/developpement/api.html#status-codes","text":"HTTP status codes are standard response codes given by web servers on the Internet. They help identify the success or failure of an API request. - 200 OK : Successful request. - 201 Created : Request was successful, and a resource was created. - 400 Bad Request : The server could not understand the request. - 401 Unauthorized : The client must authenticate itself. - 404 Not Found : The requested resource could not be found. - 500 Internal Server Error : A generic error message when an unexpected condition was encountered.","title":"Status Codes:"},{"location":"docs/developpement/api.html#requests-responses","text":"Interacting with REST APIs involves sending requests and receiving responses. The request and response formats are standardized and follow a specific structure: - Request : Comprises the endpoint, method, headers, and any data sent to the server. - Response : Includes the status code, headers, and any data sent back from the server. Understanding the foundational elements of REST APIs is crucial for effectively designing, consuming, and deploying services on the web. As we transition to building our own APIs for model deployment, this knowledge will ensure we create efficient, scalable, and user-friendly interfaces for our models.","title":"Requests &amp; Responses:"},{"location":"docs/developpement/api.html#interacting-with-rest-apis-using-python","text":"Python has a powerful library called requests that simplifies making requests to REST APIs. In this section, we will explore how to use this library to interact with an example API. For our hands-on learning, we'll fetch weather data using the requests library in Python. The requests library is a de facto standard for making HTTP requests in Python. It abstracts the complexities of making requests behind a simple API. In the following example, we'll use the requests library to fetch weather data from an API. OpenWeatherMap offers weather data, which is free for limited access. Although you typically need an API key, for brevity, we're using a mock API endpoint for our exercise.","title":"Interacting with REST APIs using Python"},{"location":"docs/developpement/api.html#crafting-a-get-request-for-weather-data","text":"We can use the requests library to make a GET request to fetch weather data from the API. We'll fetch the current weather for a city. For our example, let's use London. import requests # Mock URL for London's weather (no real API key needed) url = 'https://samples.openweathermap.org/data/2.5/weather?q=London,uk&appid=b6907d289e10d714a6e88b30761fae22' # Send the GET request response = requests.get(url) # Process and display the result if response.status_code == 200: data = response.json() print(f\"Weather in {data['name']} - {data['weather'][0]['description']}\") print(f\"Temperature: {data['main']['temp']}K\") This script will print the current weather description and temperature in Kelvin for London. Note : The data returned is in JSON format. It's structured and easy to parse in Python, which makes it a popular choice for APIs. APIs can return a lot of data. Here, besides the weather description and temperature, you can also access humidity, pressure, wind speed, and much more. Explore the data dictionary to uncover these details.","title":"Crafting a GET Request for Weather Data"},{"location":"docs/developpement/api.html#sending-data-with-post-requests-in-python","text":"While GET requests are primarily used to retrieve information from a server without causing any side effects, POST requests serve a different purpose. The POST method is designed to submit data to a server, usually resulting in a change in the server's state, such as creating a new record, updating data, or triggering an action. In essence, while GET is about asking the server for specific data, POST is about sending data to the server. With that understanding, let's delve into how to send data using POST requests in Python. For the sake of our pedagogical example, let's use JSONPlaceholder , a free fake online REST API used for testing and prototyping. Specifically, we'll be simulating the process of creating a new post. Before sending data, we must prepare it. Let's consider we're creating a new blog post: # The data we want to send post_data = { \"title\": \"Understanding REST APIs\", \"body\": \"REST APIs are fundamental in web services...\", \"userId\": 1 } This is our blog post with a title, body, and an associated user ID. With our data ready, we can send it using the requests library: import requests # The API endpoint where we want to create the new post url = 'https://jsonplaceholder.typicode.com/posts' # Sending the POST request response = requests.post(url, json=post_data) # Output the result if response.status_code == 201: print(f\"Successfully created! New Post ID: {response.json()['id']}\") else: print(\"Failed to create the post.\") Here, we've specified the URL to which we want to send the data and provided our post data in JSON format. Note : It's essential to check the response status code. A 201 status indicates that our data was successfully received and a new resource was created on the server. When you send a POST request, the server typically responds with details about the newly created resource. In our example, the server returns the ID of the newly created post, which we then print. POST requests are crucial when we want to send or update data on a server. With the requests library in Python, this process is streamlined, making data submission and interactions with web services smooth and efficient. Now that we've learned how to interact with REST APIs using Python, let's explore how to build our own API using Flask.","title":"Sending Data with POST Requests in Python"},{"location":"docs/developpement/api.html#building-a-simple-flask-api","text":"Flask is a lightweight web framework for Python, making it easy to build web applications and RESTful services. In this section, we'll set up a simple Flask API that counts API requests and provides a method to determine the number of letters in a given name. First, install Flask: pip install flask Create a new file named app.py . This will be our main application file. For our example, we'll use the following code: from flask import Flask, jsonify, request app = Flask(name) # Initialize a counter request_count = 0 @app.route('/api/count', methods=['GET']) def count(): global request_count request_count += 1 return jsonify({\"count\": request_count}) @app.route('/api/letter_count', methods=['POST']) def letter_count(): global request_count request_count += 1 data = request.json name = data.get(\"name\", \"\") return jsonify({\"name\": name, \"letter_count\": len(name)}) if name == 'main': app.run(debug=True) This code sets up a Flask application with two routes: 1. /api/count : When accessed, it increases a counter and returns the current count. 2. /api/letter_count : Accepts a POST request with a JSON payload containing a name and returns the number of letters in the name. In your terminal or command prompt, navigate to the directory containing app.py and run: python app.py The Flask server should start, and by default, it'll be accessible at http://127.0.0.1:5000/.","title":"Building a Simple Flask API"},{"location":"docs/developpement/api.html#requesting-the-flask-api-using-python","text":"With our Flask API running, let's now query it using Python: Create a new file named client.py and add the following code: import requests # Making a GET request to the count endpoint count_response = requests.get('http://127.0.0.1:5000/api/count') print(count_response.json()) # Making a POST request to the letter_count endpoint data = {\"name\": \"Alice\"} letter_count_response = requests.post('http://127.0.0.1:5000/api/letter_count', json=data) print(letter_count_response.json()) The first request will return the current request count, while the second one will tell us the number of letters in the name \"Alice\". Flask provides an intuitive way to set up RESTful APIs quickly. With just a few lines of code, we've set up a server that can handle requests, perform operations, and return data. By understanding these basics, you can extend the functionality and integrate more complex operations, such as serving machine learning models.","title":"Requesting the Flask API using Python"},{"location":"docs/developpement/colorize.html","text":"Development for Data Scientist: Practical session 2: Deploying a digit classifier Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh Data We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version. The network architecture We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine. API, Web app and deployment Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE. Solutions: model.py train.py colorize_api.py colorize_web_app.py","title":"Development for Data Scientist:"},{"location":"docs/developpement/colorize.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/developpement/colorize.html#practical-session-2-deploying-a-digit-classifier","text":"Now that you have a good understanding of the PyTorch framework and how to deploy your model through a REST API or a web application, you will develop an application that will colorize black and white images. Once again, you are expected to use Python scripts to train your model and to deploy it.","title":"Practical session 2: Deploying a digit classifier"},{"location":"docs/developpement/colorize.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. Then navigate to the developpement/colorize folder. Your working directory should look like this: code/ \u251c\u2500\u2500 data_utils.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 train.py \u251c\u2500\u2500 colorize_api.py \u251c\u2500\u2500 colorize_webapp.py \u251c\u2500\u2500 test_api.ipynb sample_images/ \u251c\u2500\u2500 img1.jpg \u251c\u2500\u2500 img2.jpg \u251c\u2500\u2500 img3.jpg \u251c\u2500\u2500 img4.jpg \u251c\u2500\u2500 img5.jpg requirements.txt download_landscapes.sh","title":"Practical session repository:"},{"location":"docs/developpement/colorize.html#data","text":"We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Run the download_landscapes.sh script to download and extract the dataset. ./download_landscapes.sh Here, we only have access to color images, so we will have to generate our own black and white images. The file data_utils.py contains some useful functions to load the dataset. In particuler given a dataset containing landscape images, the function get_colorized_dataset_loader returns a PyTorch DataLoader object that can be used to iterate over the dataset yielding batches of black and white images and their corresponding colorized version.","title":"Data"},{"location":"docs/developpement/colorize.html#the-network-architecture","text":"We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. The network architecture is defined in the unet.py file and need to be completed. Help yourself with the above image to implement a Unet network using the template located in the unet.py file: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = F.sigmoid(self.last_conv(x)) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python model.py","title":"The network architecture"},{"location":"docs/developpement/colorize.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Fill the train.py file to train a UNet to colorize images (you can inspire yourself from the one in the MNIST example. However, be careful in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'Colorize', help='experiment name') parser.add_argument('--data_path', ...) parser.add_argument('--batch_size'...) parser.add_argument('--epochs'...) parser.add_argument('--lr'...) exp_name = ... args = ... data_path = ... batch_size = ... epochs = ... lr = ... unet = UNet().to(device) loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=0) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter(f'runs/{exp_name}') train(unet, optimizer, loader, epochs=epochs, writer=writer) x, y = next(iter(loader)) with torch.no_grad(): all_embeddings = [] all_labels = [] for x, y in loader: x , y = x.to(device), y.to(device) embeddings = unet.get_features(x).view(-1, 128*28*28) all_embeddings.append(embeddings) all_labels.append(y) if len(all_embeddings)>6: break embeddings = torch.cat(all_embeddings) labels = torch.cat(all_labels) writer.add_embedding(embeddings, label_img=labels, global_step=1) writer.add_graph(unet, x.to(device)) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Some of you may have GPUs on their local machine. If that is the case, you can use them to train your model. If not, you can use Google Colab to train your model on a GPU for free. If you are using Google Colab, you are expected to do all the code development on your local machine and then send your code to collab to train your model. Try to run your code on your local machine for one or two minibatches to check that everything is working. If it is the case, you can send your code to Google Colab to train your model. To do so: - Open the run_in_colab.ipynb notebook in Google Colab. - Make sure you are connected to a GPU runtime. - Run the first cell to download the dataset. - Upload the files data_utils.py , model.py and train.py to the code folder in Google Colab. - Run the second cell to launch a tensorboard instance. - Run the third cell to launch the training. - Download the trained model and the tensorboard logs to your local machine.","title":"Training script"},{"location":"docs/developpement/colorize.html#api-web-app-and-deployment","text":"Complete the colorize_api.py file to create a Flask API that will colorize images. The API should have a /colorize endpoint that will take a black and white image as input and return the colorized version of the image. You can use the test_colorize_api.py file to test your API. You can test your app with random balck and white images from the net. For exemple one of these . You can also test your api using Postman. To do so: - Install Postman . - Launch your API. - Open Postman. - Create a new request. - Set the request type to POST. - Set the request URL to your API URL. - Go to the Body tab. - Select binary as the body type. - Select a black and white image on your computer. - Click on send. Do you have any idea why the colors are so dull? You can also complete the colorize_web_app.py file to create a web app that will colorize images. Finally complete the Dockerfile file to deploy your app on a local server. DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE.","title":"API, Web app and deployment"},{"location":"docs/developpement/colorize.html#solutions","text":"","title":"Solutions:"},{"location":"docs/developpement/colorize.html#modelpy","text":"","title":"model.py"},{"location":"docs/developpement/colorize.html#trainpy","text":"","title":"train.py"},{"location":"docs/developpement/colorize.html#colorize_apipy","text":"","title":"colorize_api.py"},{"location":"docs/developpement/colorize.html#colorize_web_apppy","text":"","title":"colorize_web_app.py"},{"location":"docs/developpement/docker.html","text":"Development for Data Scientist: Docker Course (Video by Brendan Guillouet) Slides","title":"Development for Data Scientist:"},{"location":"docs/developpement/docker.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/developpement/docker.html#docker","text":"","title":"Docker"},{"location":"docs/developpement/docker.html#course-video-by-brendan-guillouet","text":"Slides","title":"Course (Video by Brendan Guillouet)"},{"location":"docs/developpement/git_intro.html","text":"Introduction to git Slides","title":"Introduction to git"},{"location":"docs/developpement/git_intro.html#introduction-to-git","text":"Slides","title":"Introduction to git"},{"location":"docs/developpement/mnist.html","text":"Development for Data Scientist: Practical session 1: Deploying a digit classifier For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script. Practical session repository: If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer. The network class: Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x The training script The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64 Monitoring and experiment management Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network Deploying your model with Flask: Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5000 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model. Deploying your model with Gradio As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_webapp.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST? Git Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository. Docker Dockers are a way to package your application and all its dependencies in a single image that can be run on any machine. The file Dockerfile contains the instructions to build a docker image that will run your application. Build the image with the following command: sudo docker build -t mnist-flask-app . This will create a docker image named mnist-flask-app . A docker image is a read-only template that contains a set of instructions for creating a container that can run on the Docker platform. You can list all the images on your machine with the following command: sudo docker images A docker container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. Run the container with the following command: sudo docker run -p 5000:5000 mnist-flask-app You can now access your application by going to http://localhost:5000 in your browser. By defult the container will run the command python mnist_app.py --weights_path weights/mnist_net.pth when it starts. That's it! You have created a docker image that can be run on any machine that has docker installed. By doing so, you have created a reproducible environment for your application that can be run on any machine. This is very useful when you want to deploy your application on a server or in the cloud. To list all the running containers use the following command: sudo docker ps To list all the containers (running and stopped) use the following command: sudo docker ps -a To stop a container use the following command: sudo docker stop [CONTAINER_ID] To remove a container use the following command: sudo docker rm [CONTAINER_ID] To remove an image use the following command: sudo docker rmi [IMAGE_ID] DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE. Solutions: model.py train.py mnist_api.py mnist_webapp.py","title":"Development for Data Scientist:"},{"location":"docs/developpement/mnist.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/developpement/mnist.html#practical-session-1-deploying-a-digit-classifier","text":"For this session, your task is to create a script for training a simple neural network on the MNIST dataset using PyTorch. Throughout the training process, you'll utilize TensorBoard for the following purposes: Keeping an eye on your network's performance as epochs progress. Organizing your different experiments and hyperparameters. Generating visualizations to aid in analysis. Once the training process is complete, you'll also learn how to export your model in a format that can be used for inference. Finally, you will learn how to deploy your model on a REST API using Flask and how to request it using a Python script.","title":"Practical session 1: Deploying a digit classifier"},{"location":"docs/developpement/mnist.html#practical-session-repository","text":"If you haven't already done so, create an account on Github . Then fork this repository and clone it on your computer.","title":"Practical session repository:"},{"location":"docs/developpement/mnist.html#the-network-class","text":"Using the figure above, fill in the following code, in the model.py file, to create the network class: The method __init__() should instantiate all the layers that will be used by the network. The method forward() describes the forward graph of your network. All the pooling operations and activation functions are realized in this method. Do not forget to change the shape of your input before the first linear layer using torch.flatten(...) or x.view(...) . import torch import torch.nn as nn import torch.nn.functional as F class MNISTNet(nn.Module): def __init__(self): super(MNNISTNet, self).__init__() self.conv1 = nn.Conv2d(...) self.conv2 = nn.Conv2d(...) self.pool = nn.MaxPool2d(...) self.fc1 = nn.Linear(...) self.fc2 = nn.Linear(...) self.fc3 = nn.Linear(...) def forward(self, x): x = F.relu(self.conv1(x)) # First convolution followed by x = self.pool(x) # a relu activation and a max pooling# x = ... ... x = self.fc3(x) return x","title":"The network class:"},{"location":"docs/developpement/mnist.html#the-training-script","text":"The earlier file included your model class. Now, you will proceed to finalize the training script, named train.py . This script will serve as a Python script for training a neural network on the MNIST Dataset. Both the train() and test() methods have already been implemented. import argparse from statistics import mean import torch import torchvision import torchvision.transforms as transforms import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from tqdm import tqdm from model import MNISTNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') def test(model, dataloader): test_corrects = 0 total = 0 with torch.no_grad(): for x, y in dataloader: x = x.to(device) y = y.to(device) y_hat = model(x).argmax(1) test_corrects += y_hat.eq(y).sum().item() total += y.size(0) return test_corrects / total Now you will implement the main method, which will be executed each time the Python script is run. You'd like to offer users the flexibility to adjust certain learning process parameters, specifically: Batch size Learning rate Number of training epochs To achieve this, the Python argparse module will be employed. This module simplifies the creation of user-friendly command-line interfaces. Incorporating arguments into a Python script through argparse is a straightforward process. To begin, you'll need to import the argparse module and create a parser instance within the main method: import argparse if __name__=='__main__': parser = argparse.ArgumentParser() Then, just add a new argument to the parser precising the argument's name, its type, and optionaly a default value and an helping message. parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') Finaly, you can use the arguments as follows: args = parser.parse_args() print(args.exp_name) Complete the main method to parse the four possible arguments provided when executing the script: if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--exp_name', type=str, default = 'MNIST', help='experiment name') parser.add_argument(...) parser.add_argument(...) parser.add_argument(...) args = parser.parse_args() exp_name = args.exp_name epochs = ... batch_size = ... lr = ... The following code instantiates two data loaders : one loading data from the training set, the other one from the test set. # transforms transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]) # datasets trainset = torchvision.datasets.MNIST('./data', download=True, train=True, transform=transform) testset = torchvision.datasets.MNIST('./data', download=True, train=False, transform=transform) # dataloaders trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) Instantiate a MNISTNet and a SGD optimizer using the learning rate provided in the script arguments. Call the train method to train your network and the test method to evaluate it. Finally, print the test accuracy. net = ... # setting net on device(GPU if available, else CPU) net = net.to(device) optimizer = optim.SGD(...) train(...) test_acc = test(...) print(f'Test accuracy:{test_acc}') Save your model using the torch.save method. This method takes two arguments: the first one is the object to save, the second one is the path to the file where the object will be saved. Here, you will save the model's state dictionary ( net.state_dict() ) in a file named mnist_net.pth . The state dictionary is a Python dictionary containing all the weights and biases of the network. torch.save(net.state_dict(), 'weights/mnist_net.pth') You should now be able to run your python script using the following command in your terminal: python train.py --epochs=5 --lr=1e-3 --batch_size=64","title":"The training script"},{"location":"docs/developpement/mnist.html#monitoring-and-experiment-management","text":"Training our model on MNIST is pretty fast. Nonetheless, in most cases, training a network may be very long. For such cases, it is essential to log partial results during training to ensure that everything is behaving as expected. A very famous tool to monitor your experiments in deep learning is Tensorboard. The main object used by Tensorboard is a SummaryWriter . Add the following import: from torch.utils.tensorboard import SummaryWriter and modify the train method to take an additional argument named writer . Use its add_scalar method to log the training loss for every epoch. def train(net, optimizer, loader, writer, epochs=10): criterion = nn.CrossEntropyLoss() for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: x, y = x.to(device), y.to(device) outputs = net(x) loss = criterion(outputs, y) running_loss.append(loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() t.set_description(f'training loss: {mean(running_loss)}') writer.add_scalar('training loss', mean(running_loss), epoch) In the main method instantiate a SummaryWriter with writer = SummaryWriter(f'runs/MNIST') and add it as argument to the train method. Re-run your script and check your tensorboard logs using in a separate terminal: tensorboard --logdir runs You can use tensorboard to log many different things such as your network computational graph, images, samples from your dataset, embeddings, or even use it for experiment management. Add a new method to the MNISTNet class to get the embeddings computed after the last convolutional layer. def get_features(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16 * 4 * 4) return x Now these following code to the end of your main function to log the embeddings and the computational graph in tensorboard. #add embeddings to tensorboard perm = torch.randperm(len(trainset.data)) images, labels = trainset.data[perm][:256], trainset.targets[perm][:256] images = images.unsqueeze(1).float().to(device) with torch.no_grad(): embeddings = net.get_features(images) writer.add_embedding(embeddings, metadata=labels, label_img=images, global_step=1) # save networks computational graph in tensorboard writer.add_graph(net, images) # save a dataset sample in tensorboard img_grid = torchvision.utils.make_grid(images[:64]) writer.add_image('mnist_images', img_grid) Re-run your script and restart tensorboard. Visualize the network computational graph by clicking on Graph . You should see something similar to this: Click on the inactive button and choose projector to look at the embeddings computed by your network","title":"Monitoring and experiment management"},{"location":"docs/developpement/mnist.html#deploying-your-model-with-flask","text":"Now that your model is trained, you will deploy it using a simple Flask application. Flask is a micro web framework written in Python. It is classified as a microframework because it does not require particular tools or libraries. The following code is a simple Flask application that will load your model and given an image, it will return the predicted class. The application will listen on port 5000 and will have a single route /predict that will accept a POST request with an image as payload. The image will be received as a byte stream and will first be converted to a PIL image, then will be transformed using the same transformation as during training to be fed to the model. The model will return a tensor containing the probabilities for each class. The class with the highest probability will be returned as a JSON object. Complete the following code to take the path of your model as an argument and load it in the model variable. import argparse import torch import torchvision.transforms as transforms from flask import Flask, jsonify, request from PIL import Image import io from models import MNISTNet device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') app = Flask(__name__) parser = ... ... model_path = ... model = MNISTNet().to(device) # Load the model model.load_state_dict(torch.load(model_path)) model.eval() transform = transforms.Compose([ transforms.Resize((28, 28)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)) ]) @app.route('/predict', methods=['POST']) def predict(): img_binary = request.data img_pil = Image.open(io.BytesIO(img_binary)) # Transform the PIL image tensor = transform(img_pil).to(device) tensor = tensor.unsqueeze(0) # Add batch dimension # Make prediction with torch.no_grad(): outputs = model(tensor) _, predicted = outputs.max(1) return jsonify({\"prediction\": int(predicted[0])}) if __name__ == \"__main__\": app.run(debug=True) Save the code in a file named mnist_api.py and run it with: python mnist_api.py --model_path [PATH_TO_YOUR_MODEL] Now run the test_api.ipynb notebook to test your API. We requested the api one image at a time. As you may already know, neural networks are much more efficient when they are fed with a batch of images. Modify the mnist_api.py by adding a new route /batch_predict that will accept a batch of images and return a batch of predictions and test it with the last cell of the test_api.ipynb notebook. @app.route('/batch_predict', methods=['POST']) def batch_predict(): # Get the image data from the request images_binary = request.files.getlist(\"images[]\") tensors = [] for img_binary in images_binary: img_pil = Image.open(img_binary.stream) tensor = transform(img_pil) tensors.append(tensor) # Stack tensors to form a batch tensor batch_tensor = torch.stack(tensors, dim=0) # Make prediction with torch.no_grad(): outputs = model(batch_tensor.to(device)) _, predictions = outputs.max(1) return jsonify({\"predictions\": predictions.tolist()}) ``` ## A simple GUI with tkinter The file `mnist_gui.py` contains a simple GUI that will allow you to draw a digit and send it to the API to get a prediction. Run the script with: ```bash python mnist_gui.py --model_path [PATH_TO_YOUR_MODEL] and provide some of the images in the MNIST_sample folder as input to your model.","title":"Deploying your model with Flask:"},{"location":"docs/developpement/mnist.html#deploying-your-model-with-gradio","text":"As you can see, the GUI is very simple and not very user friendly. Gradio is a library that allows you to quickly create a user friendly web interface for your model. Install the library: pip install gradio Creating an application with Gradio is done through the use of its Interface class The core Interface class is initialized with three required parameters: fn: the function to wrap a user interface around inputs: which component(s) to use for the input, e.g. \"text\" or \"image\" or \"audio\" outputs: which component(s) to use for the output, e.g. \"text\" or \"image\" \"label\" Gradio includes more than 20 different components , most of which can be used as inputs or outputs. In this example, we will use a sketchpad (which is an instance of the Image component )component for the input and a Label component for the output. gr.Interface(fn=recognize_digit, inputs=\"sketchpad\", outputs='label', live=True, description=\"Draw a number on the sketchpad to see the model's prediction.\", ).launch(debug=True, share=True); Complete the mnist_webapp.py to either load your model weights or use your api to perform the predictions and run your app with the following command: python mnist_app.py --weights_path [path_to_the weights] Is your model accurate with your drawings? Do you know why it is less accurate than on MNIST?","title":"Deploying your model with Gradio"},{"location":"docs/developpement/mnist.html#git","text":"Commit all the modifications you have made to the repository as well as the weights and push them to your remote repository.","title":"Git"},{"location":"docs/developpement/mnist.html#docker","text":"Dockers are a way to package your application and all its dependencies in a single image that can be run on any machine. The file Dockerfile contains the instructions to build a docker image that will run your application. Build the image with the following command: sudo docker build -t mnist-flask-app . This will create a docker image named mnist-flask-app . A docker image is a read-only template that contains a set of instructions for creating a container that can run on the Docker platform. You can list all the images on your machine with the following command: sudo docker images A docker container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. Run the container with the following command: sudo docker run -p 5000:5000 mnist-flask-app You can now access your application by going to http://localhost:5000 in your browser. By defult the container will run the command python mnist_app.py --weights_path weights/mnist_net.pth when it starts. That's it! You have created a docker image that can be run on any machine that has docker installed. By doing so, you have created a reproducible environment for your application that can be run on any machine. This is very useful when you want to deploy your application on a server or in the cloud. To list all the running containers use the following command: sudo docker ps To list all the containers (running and stopped) use the following command: sudo docker ps -a To stop a container use the following command: sudo docker stop [CONTAINER_ID] To remove a container use the following command: sudo docker rm [CONTAINER_ID] To remove an image use the following command: sudo docker rmi [IMAGE_ID] DO NOT FORGET TO DELETE YOUR DOCKER IMAGE AND CONTAINER WHEN YOU ARE DONE.","title":"Docker"},{"location":"docs/developpement/mnist.html#solutions","text":"","title":"Solutions:"},{"location":"docs/developpement/mnist.html#modelpy","text":"","title":"model.py"},{"location":"docs/developpement/mnist.html#trainpy","text":"","title":"train.py"},{"location":"docs/developpement/mnist.html#mnist_apipy","text":"","title":"mnist_api.py"},{"location":"docs/developpement/mnist.html#mnist_webapppy","text":"","title":"mnist_webapp.py"},{"location":"docs/developpement/pytorch.html","text":"Development for Data Scientist: Pytorch and Python Script Course Course notebook: Notebook","title":"Development for Data Scientist:"},{"location":"docs/developpement/pytorch.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/developpement/pytorch.html#pytorch-and-python-script","text":"","title":"Pytorch and Python Script"},{"location":"docs/developpement/pytorch.html#course","text":"","title":"Course"},{"location":"docs/developpement/pytorch.html#course-notebook","text":"Notebook","title":"Course notebook:"},{"location":"docs/developpement/quizz.html","text":"Quizz: Developpement tools for Data Scientist Chargement\u2026","title":"Quizz: Developpement tools for Data Scientist"},{"location":"docs/developpement/quizz.html#quizz-developpement-tools-for-data-scientist","text":"Chargement\u2026","title":"Quizz: Developpement tools for Data Scientist"},{"location":"docs/nlp/nlp.html","text":"Introduction to natural language processing Slides Practical session Practical session Solution Project: During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Introduction to natural language processing"},{"location":"docs/nlp/nlp.html#introduction-to-natural-language-processing","text":"Slides","title":"Introduction to natural language processing"},{"location":"docs/nlp/nlp.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"docs/nlp/nlp.html#project","text":"During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Project:"},{"location":"docs/nlp/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"docs/old/docker_2021.html","text":"Development for Data Scientist: Docker Course Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Development for Data Scientist:"},{"location":"docs/old/docker_2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/old/docker_2021.html#docker","text":"","title":"Docker"},{"location":"docs/old/docker_2021.html#course","text":"Slides","title":"Course"},{"location":"docs/old/docker_2021.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Practical Session"},{"location":"docs/old/gcloud2021.html","text":"Development for Data Scientist: Introduction to Google Cloud Computing Course Slides Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it! Set up your virtual machine First follow the GCloud setup process described here . The python script Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Training on GCloud You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Bonus synchronize with Rsync An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Development for Data Scientist:"},{"location":"docs/old/gcloud2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"docs/old/gcloud2021.html#introduction-to-google-cloud-computing","text":"","title":"Introduction to Google Cloud Computing"},{"location":"docs/old/gcloud2021.html#course","text":"Slides","title":"Course"},{"location":"docs/old/gcloud2021.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it!","title":"Practical Session"},{"location":"docs/old/gcloud2021.html#set-up-your-virtual-machine","text":"First follow the GCloud setup process described here .","title":"Set up your virtual machine"},{"location":"docs/old/gcloud2021.html#the-python-script","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The","title":"The python script"},{"location":"docs/old/gcloud2021.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth')","title":"Training script"},{"location":"docs/old/gcloud2021.html#training-on-gcloud","text":"You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab.","title":"Training on GCloud"},{"location":"docs/old/gcloud2021.html#bonus-synchronize-with-rsync","text":"An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Bonus synchronize with Rsync"},{"location":"docs/old/gcloud_set_up.html","text":"Set up GCloud: Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"docs/old/gcloud_set_up.html#set-up-gcloud","text":"Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"docs/old/policy_gradient.html","text":"Introduction to Reinforcement Learning: Policy Gradient Slides Practical session","title":"Introduction to Reinforcement Learning:"},{"location":"docs/old/policy_gradient.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"docs/old/policy_gradient.html#policy-gradient","text":"Slides Practical session","title":"Policy Gradient"},{"location":"docs/old/q_learning.html","text":"Introduction to Reinforcement Learning: From policy iteration to Deep Q-Learning Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"Introduction to Reinforcement Learning:"},{"location":"docs/old/q_learning.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"docs/old/q_learning.html#from-policy-iteration-to-deep-q-learning","text":"Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"From policy iteration to Deep Q-Learning"},{"location":"docs/old/text1.html","text":"Text Cleaning and Text Vectorization Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"docs/old/text1.html#text-cleaning-and-text-vectorization","text":"Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"docs/old/text2.html","text":"Words Embedding Slides Practical session","title":"Words Embedding"},{"location":"docs/old/text2.html#words-embedding","text":"Slides Practical session","title":"Words Embedding"},{"location":"docs/old/text3.html","text":"Text Recurrent Network Slides Practical session","title":"Text Recurrent Network"},{"location":"docs/old/text3.html#text-recurrent-network","text":"Slides Practical session","title":"Text Recurrent Network"},{"location":"docs/project/Assignment.html","text":"Project assignment You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"docs/project/Assignment.html#project-assignment","text":"You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"docs/project/Docker_compose.html","text":"Docker compose example Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want. How to run Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository","title":"Docker compose example"},{"location":"docs/project/Docker_compose.html#docker-compose-example","text":"Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want.","title":"Docker compose example"},{"location":"docs/project/Docker_compose.html#how-to-run","text":"Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository","title":"How to run"},{"location":"docs/rec_sys/quizz.html","text":"Quizz: Recommender systems Chargement\u2026","title":"Quizz: Recommender systems"},{"location":"docs/rec_sys/quizz.html#quizz-recommender-systems","text":"Chargement\u2026","title":"Quizz: Recommender systems"},{"location":"docs/rec_sys/rec_sys.html","text":"Recommendation Systems Course: Slides recommender systems Practical session: IMDB recommender system: Solution: Project: During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. An easy way to run both the web app and the annoy index in a single container would consist to create a bash script that runs both the web app and the annoy index. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the annoy index: #!/bin/bash python python annoy_db.py & gradio_app.py The & operator is used to put jobs in the background. Here the annoy index is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the annoy index in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the annoy index in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the Annoy database. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - annoy-db annoy-db: build: context: . dockerfile: Dockerfile-annoy ports: - \"5000:5000\" Make sure in your gradion app to call the annoy database through the url http://annoy-db:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The Annoy database API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down","title":"Recommendation Systems"},{"location":"docs/rec_sys/rec_sys.html#recommendation-systems","text":"","title":"Recommendation Systems"},{"location":"docs/rec_sys/rec_sys.html#course","text":"Slides recommender systems","title":"Course:"},{"location":"docs/rec_sys/rec_sys.html#practical-session","text":"IMDB recommender system: Solution:","title":"Practical session:"},{"location":"docs/rec_sys/rec_sys.html#project","text":"During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. An easy way to run both the web app and the annoy index in a single container would consist to create a bash script that runs both the web app and the annoy index. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the annoy index: #!/bin/bash python python annoy_db.py & gradio_app.py The & operator is used to put jobs in the background. Here the annoy index is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the annoy index in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the annoy index in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the Annoy database. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - annoy-db annoy-db: build: context: . dockerfile: Dockerfile-annoy ports: - \"5000:5000\" Make sure in your gradion app to call the annoy database through the url http://annoy-db:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The Annoy database API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down","title":"Project:"},{"location":"docs/rl/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"docs/rl/rl.html","text":"Introduction to Reinforcement Learning: Slides Practical sessions: , solution: , solution: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"docs/rl/rl.html#introduction-to-reinforcement-learning","text":"Slides Practical sessions: , solution: , solution: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"docs/xai/interpretability.html","text":"Interpretability in Machine Learning Slides Practical session Practical session Solution","title":"Interpretability in Machine Learning"},{"location":"docs/xai/interpretability.html#interpretability-in-machine-learning","text":"Slides","title":"Interpretability in Machine Learning"},{"location":"docs/xai/interpretability.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"docs/xai/project.html","text":"XAI Project. This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"docs/xai/project.html#xai-project","text":"This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"docs/xai/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"nlp/nlp.html","text":"Introduction to natural language processing Slides Practical session Practical session Solution Project: During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Course and practical session"},{"location":"nlp/nlp.html#introduction-to-natural-language-processing","text":"Slides","title":"Introduction to natural language processing"},{"location":"nlp/nlp.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"nlp/nlp.html#project","text":"During the practical session, you saw how to compute embeddings of documents using three techniques: Bag_of_words, Word2Vec and BERT. You will now build a recommender system based on content using the movie plots. To do so get the movies_metadata.csv file from here and compute the embeddings of each movie plot (the overview column) using at least a bag-of_word technique and a pre-trained model (Glove or DistillBert). You sould create one new column for each embedding technique. Once this is done, build an annoy index for each embedding. Similarly to the recommender system project, I want you to build a web app that takes a movie description as input and returns the 5 most similar movies according to their plot. The web app should be light and fast and provide the possibility to choose the embedding technique to use.","title":"Project:"},{"location":"nlp/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"old/docker_2021.html","text":"Development for Data Scientist: Docker Course Slides Practical Session In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Development for Data Scientist:"},{"location":"old/docker_2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"old/docker_2021.html#docker","text":"","title":"Docker"},{"location":"old/docker_2021.html#course","text":"Slides","title":"Course"},{"location":"old/docker_2021.html#practical-session","text":"In this practical session, you will now run your code through a Docker container. Using docker in data science projects has two advantages: * Improving the reproducibility of the results * Facilitating the portability and deployment In this session, we will try to package the code from the previous session, allowing us to train a neural network to colorize images into a Docker image and use this image to instantiate a container on a GCloud instance to run the code. We will first create the Dockerfile corresponding to our environment. On your local machine, create a new file named Dockerfile containing the following code: # Base image from pytorch FROM pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime # Set up for your local zone an UTC information ENV TZ=Europe/Paris RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone # Additional librairies RUN pip install tqdm tensorboard Take a moment to analyze this dockerfile. As you can see, it is built upon an existing image from Pytorch. Starting from existing images allows for fast prototyping. You may find existing images on DockerHub . The Pytorch image we will be using is available here Fire up your GCloud instance and send your dockerfile using gcloud compute scp [your_file_path] [your_instance_name]:Workspace/ --zone [your_instance_zone] Connect to your instance: gcloud compute ssh --zone [your_instance_zone] [your_instance_name] If docker is not already installed in your machine, follow this guide to install it. You will also need the NVIDIA Container Toolkit to be installed to allow docker to communicate with the instance GPU. If you created your GCloud instance following the previous session's instructions, it should be OK. To verify that it is OK you may run the following command: sudo docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi If the command output is in the form of : +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.51.06 Driver Version: 450.51.06 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla T4 On | 00000000:00:1E.0 Off | 0 | | N/A 34C P8 9W / 70W | 0MiB / 15109MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then everything is OK. Otherwise, you may need to install the NVIDIA Container Toolkit manually, following this guide . You may now build your first image using the following command: sudo docker build -t [your_image_name] -f [path_to_your_image] [build_context_folder] The image should take a few minutes to build. Once this is done, use the following command to list the available images on your GCloud instance: sudo docker image ls How many images can you see? What do they refer to? Now that our images are built, we can now use them to instantiate containers. Since a container is an instance of an image, we can instantiate several containers using a single image. We will run our first container using the interactive mode. Run the following command to run your fist container: docker run -it --name [your_container_name] [your_image_name] You should now have access to an interactive terminal from your container. On this terminal, open a Python console and check that Pytorch is installed and has access to your instance GPU. import torch print(torch.cuda.is_available()) Quit the Python console and quit your container using ctrl+d . You can list all your running containers using the following command: sudo docker container ls Your container is closed and does not appear. To list all the existing containers, add the -a to the previous command. sudo docker container ls -a Start your containers using: sudo docker start [container_id_or_name] Check that it is now listed as started. You can have access to its interactive mode using the attach command: sudo docker attach [container_id_or_name] You can delete a container using the rm command: sudo docker rm [container_id_or_name] We will now see how to share data between the container and the machine it is running on. First create a folder containing the files: * download_landscapes.sh * unet.py * colorize.py * data_utils.py Create a new container using this time mounting a shared volume using the following command: docker run -it --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Go to the shared folder and run the download_landscapes.sh script. Leave the container and look at your folder in the local. What can you see? If you want to run your job using the interactive mode, you need to give access at your container to your host resources. Start a new container using the following command to get access to the GPU and CPU resources and run the colorize.py script. docker run -it --gpus all --ipc=host --name [container_name] -v ~/[your_folder_path]:/workspace/[folder_name] [image_name] Now try to send all the results and weights to your local machine and maybe to look at the tensorboard logs.","title":"Practical Session"},{"location":"old/gcloud2021.html","text":"Development for Data Scientist: Introduction to Google Cloud Computing Course Slides Practical Session In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it! Set up your virtual machine First follow the GCloud setup process described here . The python script Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The Training script You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth') Training on GCloud You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab. Bonus synchronize with Rsync An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Development for Data Scientist:"},{"location":"old/gcloud2021.html#development-for-data-scientist","text":"","title":"Development for Data Scientist:"},{"location":"old/gcloud2021.html#introduction-to-google-cloud-computing","text":"","title":"Introduction to Google Cloud Computing"},{"location":"old/gcloud2021.html#course","text":"Slides","title":"Course"},{"location":"old/gcloud2021.html#practical-session","text":"In this session, you will train a neural network to colorize black and white images using virtual machines on Google Cloud . You will have to: Set up a new GCloud instance with GPU capacities Write your Python scripts on your local machine Send your code to your GCloud Instance Run your code on the cloud virtual machine Monitor your code running on the virtual machine Get your results and send them to your local machine The solution is available here. Try to complete the practical session without looking at it!","title":"Practical Session"},{"location":"old/gcloud2021.html#set-up-your-virtual-machine","text":"First follow the GCloud setup process described here .","title":"Set up your virtual machine"},{"location":"old/gcloud2021.html#the-python-script","text":"Cloud providers charge by the hour, so cloud computing can quickly get expensive. A good practice consists of doing most of the code development on your local hardware before sending it to your cloud instances. That is what you are going to do in this practical session. You will run one small iteration of your code on your local machine to test your code and then send it to your virtual machine. We will be working with the Landscapes dataset composed of 4000 images in seven categories of landscapes (city, road, mountain, lake, ocean, field, and forest). Instead of using it to train a classifier, we will use it to train a neural network to colorize black and white images. Create a script download_landscapes.sh with the following content and execute it to download and extract the dataset. cd data wget https://github.com/ml5js/ml5-data-and-models/raw/master/datasets/images/landscapes/landscapes_small.zip mkdir landscapes unzip landscapes_small.zip -d landscapes rm landscapes_small.zip rm -r landscapes/__MACOSX cd .. We will use a particular category of neural networks to perform the colorization operation: Unets . Initially designed for Biomedical Image Segmentation, Unets offer state-of-the-art performances in many segmentation tasks. These performances are mainly due to the skip connections used in UNets architectures. Indeed, Unets are a particular form of Auto-Encoders using skip connections between corresponding layers of the encoder and the decoder. Create a new file named unet.py where you will define the following Unet network: Help yourself with the above image to implement a Unet network using the following template: import torch import torch.nn as nn import torch.nn.functional as F def double_conv(in_channels, out_channels): # returns a block compsed of two Convolution layers with ReLU activation function return nn.Sequential( nn.Conv2d(in_channels, out_channels, 3, padding=1), nn.ReLU(), nn.Conv2d(out_channels, out_channels, 3, padding=1), nn.ReLU() ) class DownSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.maxpool = ... def forward(self, x): x_skip = ... out = ... return out , x_skip class UpSampleBlock(nn.Module): def __init__(self, in_channels, out_channels): super().__init__() self.conv_block = ... self.upsample = ... # use nn.Upsample def forward(self, x, x_skip): x = self.upsample(x) x = torch.cat([x, x_skip], dim=1) # concatenates x and x_skip x = self.conv_block(x) return x class UNet(nn.Module): def __init__(self): super().__init__() self.downsample_block_1 = ... self.downsample_block_2 = ... self.downsample_block_3 = ... self.middle_conv_block = double_conv(128, 256) self.upsample_block_3 = ... self.upsample_block_2 = ... self.upsample_block_1 = ... self.last_conv = nn.Conv2d(32, 3, 1) def forward(self, x): x, x_skip1 = ... x, x_skip2 = ... x, x_skip3 = ... x = self.middle_conv_block(x) x = #use upsampleblock_3 and x_skip3 x = #use upsampleblock_2 and x_skip2 x = #use upsampleblock_1 and x_skip1 out = self.(x) return out if __name__=='__main__': x = torch.rand(16,1,224,224) net = UNet() y = net(x) assert y.shape == (16,3,224,224) print('Shapes OK') Check that your network is producing correct outputs by running your file with: python unet.py The","title":"The python script"},{"location":"old/gcloud2021.html#training-script","text":"You will now implement the training procedure. Training a network to colorize images is a supervised regression problem. Consider \\(x\\) a grayscaled image and \\(y\\) its corresponding colored image. Training a parametrized network \\(f_\\theta\\) to predict colorized images \\(\u0177\\) amounts to minimizing the distance between the prediction \\(\u0177\\) and the actual \\(y\\) . That is to say minimizing \\(MSE(y, f_\\theta(x))\\) . Create a new file data_utils.py that will handle the dataset: from torchvision.datasets.folder import ImageFolder, default_loader, IMG_EXTENSIONS from torch.utils.data import DataLoader import torchvision.transforms as transforms class ImageFolderGrayColor(ImageFolder): def __init__( self, root, transform=None, target_transform=None, ): super(ImageFolder, self).__init__(root=root, loader=default_loader, transform=transform, extensions=IMG_EXTENSIONS, target_transform=target_transform) #TODO \u00e0 modifier def __getitem__(self, index): \"\"\" Args: index (int): Index Returns: tuple: (sample, target) where target is class_index of the target class. \"\"\" path, _ = self.samples[index] sample = self.loader(path) if self.target_transform is not None: target = self.target_transform(sample) if self.transform is not None: sample = self.transform(sample) return sample, target def get_colorized_dataset_loader(path, **kwargs): source_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.Grayscale(num_output_channels=1), transforms.ToTensor(), transforms.Normalize(mean=[0.5], std=[0.5])]) target_process = transforms.Compose( [transforms.Resize((224, 224)), transforms.ToTensor()]) dataset = ImageFolderGrayColor(path, source_process, target_process) return DataLoader(dataset, **kwargs) Create a new file colorize.py and fill the train method in the following canvas (you can inspire yourself from the one in the MNIST example. Be careful, however, in your criterion choice): import argparse # to parse script arguments from statistics import mean # to compute the mean of a list from tqdm import tqdm #used to generate progress bar during training import torch import torch.optim as optim from torch.utils.tensorboard import SummaryWriter from torchvision.utils import make_grid #to generate image grids, will be used in tensorboard from data_utils import get_colorized_dataset_loader # dataloarder from unet import UNet # setting device on GPU if available, else CPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') def train(net, optimizer, loader, epochs=5, writer=None): criterion = ... for epoch in range(epochs): running_loss = [] t = tqdm(loader) for x, y in t: # x: black and white image, y: colored image ... ... ... ... ... ... ... ... if writer is not None: #Logging loss in tensorboard writer.add_scalar('training loss', mean(running_loss), epoch) # Logging a sample of inputs in tensorboard input_grid = make_grid(x[:16].detach().cpu()) writer.add_image('Input', input_grid, epoch) # Logging a sample of predicted outputs in tensorboard colorized_grid = make_grid(outputs[:16].detach().cpu()) writer.add_image('Predicted', colorized_grid, epoch) # Logging a sample of ground truth in tensorboard original_grid = make_grid(y[:16].detach().cpu()) writer.add_image('Ground truth', original_grid, epoch) return mean(running_loss) if __name__=='__main__': parser = argparse.ArgumentParser() parser.add_argument('--data_path', type=str, default = 'data/landscapes', help='dataset path') parser.add_argument('--batch_size', type=int, default = int(32), help='batch_size') parser.add_argument('--epochs', type=int, default = int(10), help='number of epochs') parser.add_argument('--lr', type=float, default = float(1e-3), help='learning rate') args = parser.parse_args() data_path = args.data_path batch_size = args.batch_size epochs = args.epochs lr = args.lr unet = UNet().cuda() loader = get_colorized_dataset_loader(path=data_path, batch_size=batch_size, shuffle=True, num_workers=4) optimizer = optim.Adam(unet.parameters(), lr=lr) writer = SummaryWriter('runs/UNet') train(unet, optimizer, loader, epochs=epochs, writer=writer) writer.add_graph(unet) # Save model weights torch.save(unet.state_dict(), 'unet.pth')","title":"Training script"},{"location":"old/gcloud2021.html#training-on-gcloud","text":"You now have everything to run your code on GCloud. Fire up your GCloud instance. On a terminal, connect to your instance using the following command (replace the zone and instance name with yours): gcloud compute ssh --zone \"europe-west1-d\" \"your_instance_name\" Create a folder Workspace on your virtual machine: mkdir Workspace You can copy a file from your local machine to the virtual machine using the following command on your local terminal: gcloud compute scp [your_file_path] your_instance_name:Workspace/ --zone \"europe-west1-d\" Conversly, you can copy a from your virtual machine to your local machine the following command on your local terminal: gcloud compute scp bsf.pth your_instance_name:Workspace/ --zone \"europe-west1-d\" Add the --recurse argument to your command if you want to copy a folder. Copy the folder containing all your code into your virtual machine's Workspace folder. Also, copy the download_landscapes.sh file into your VM and execute it. You should now be able to run your python script and thus learn to colorize images. Run your script for one entire epoch to check that everything is working fine. We will now run our script for a few more epochs, but before that, we will create an ssh tunnel between our local machine and the virtual machine. Run the following command on your local machine with the correct name for your virtual machine (use your correct zone). gcloud compute ssh --ssh-flag=\"-L 8898:localhost:8898\" --zone \"us-central1-b\" \"example_instance_name\" This command connects you to your virtual machine and forwards its port 8898 to your local machine's port 8898 (you can change the port value if needed). Thanks to that, you will get access to the tensorboard interface running on the virtual machine through your local machine. Now on this terminal window, run the following command: tensorboard --logdir runs --port 8898 On another terminal, connect to your virtual machine and run your script with a few more epochs (like 10 for instance). On your web browser, go to the following adress: http://localhost:8898/ You should be able to access tensorboard. Check on your network graph. You should see the U shape of your Unet. You can now visualize the progression of your network while it is training in the images tab.","title":"Training on GCloud"},{"location":"old/gcloud2021.html#bonus-synchronize-with-rsync","text":"An easy way to synchronize your code with your VM is to use rsync Install rsync on your virtual machine : sudo apt-get install rsync Add the public key you\u2019re going to use to connect to the VM to the VM\u2019s ~/.ssh/authorized_keys On the virtual machine: touch ~/.ssh/authorized_keys nano ~/.ssh/authorized_keys Copy the content of your public key (It is usually located in ~/.ssh/id_rsa.pub on your local machine) in the opened file. Press ctrl+x then y then enter to save. Now to synchronize a folder, find the IP of your virtual machine in the GCloud interface: To synchronize your folder on your local machine (for instance, named test_gcloud ) with a distant folder on the virtual machine (located, for example, in Workspace/test_gcloud): rsync -r [VM_IP_ADRESS]:Workspace/test_gcloud/ test_gcloud/ To synchronize a distant folder on the virtual machine with a folder on your local machine : rsync -r test_gcloud/ [VM_IP_ADRESS]:Workspace/test_gcloud/ You can find more information on the rsync command here (in french) You can stop your VM using the GCloud interface or just by running the following command: sudo shutdown -h now","title":"Bonus synchronize with Rsync"},{"location":"old/gcloud_set_up.html","text":"Set up GCloud: Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"old/gcloud_set_up.html#set-up-gcloud","text":"Google sponsors this course with free GCloud credits through the Cloud Higher Education Programs. Go to this link to claim your coupon code for the credits associated with this course. Once you have your coupon code go to this link to get your credits (you will need a Google account, if needed, you can create one using your INSA mail address). Once you are on the GCloud homepage, start by creating a new project: Once your project is created go to Compute Engine -> VM instances and activate Compute Engine API for your project . You now need to add GPU capacity to your project. Go to IAM and admin -> Quotas . On the filter type the following Quota:GPUs (all regions) select the resulting service and click on modify quotas . Increase the limit to one and add a short description to your request: Hi, I am a student enrolled at INSA Toulouse. The GCloud Higher education program granted us free credits. These credits are associated with a course in which the practical sessions will require access to GPU machines. Would you please increase my GPU quota so I can participate to the practical sessions? Many thanks, You name This process may take some time. Therefore, be sure sure to complete every steps at least a few days before the practical session. You will also need to install the Cloud SDK Command-line interface. It should already be installed on the INSA's machines. If you are using your personal computer, follow the corresponding installation procedure available here . Once the GCloud SDK is configured on your local machine, go to your GCloud interface and go to Compute engine ->VM instances Click on the create new instance button to create your first instance. Be sure at this point that your quota request has been approved or you will not be able to attach a GPU to your Virtual Machine. Now create a new instance on the same region you asked for your GPU quota. You may follow the following parameters settings for the practical session. (click on plate-forme du CPU et GPU and add a K80 GPU) Select the following hard drive options Finally, check the two checkboxes at the bottom of the page to allow Http/Https traffic. After a few minutes your instance should be created. It should appear in the VM instance panel: If its status is in green, the virtual machine is started and is now consuming your free credit. Go to ssh -> show ssh the gcloud command and copy the command on your terminal (you may need to run the gcloud init command before if it does not work at first). You should now be connected to your virtual machine and see the following output on your terminal: Type 'y' to install the nvidia drivers. If the drivers failed to be installed and you obtain the following message: Wait a few minutes and type: cd /opt/deeplearning/ sudo ./install-driver.sh To verify that everything is correctly installed type the following command nvidia-smi If you see something like +-----------------------------------------------------------------------------+ | NVIDIA-SMI 460.73.01 Driver Version: 460.73.01 CUDA Version: 11.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla K80 Off | 00000000:00:04.0 Off | 0 | | N/A 36C P0 69W / 149W | 0MiB / 11441MiB | 100% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | No running processes found | +-----------------------------------------------------------------------------+ then your installation is almost complete. Run the following commands to install the libraries needed for the practical session: pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu113 pip install tqdm gradio tensorboard Everything should be OK now. You can stop your virtual machine until the practical session using the GCloud web interface or typing the following command: sudo shutdown -h now Be sur to do all this process before the practical session, it will save you many time and remember at least to ask for your quotas as soon as possible!","title":"Set up GCloud:"},{"location":"old/policy_gradient.html","text":"Introduction to Reinforcement Learning: Policy Gradient Slides Practical session","title":"Introduction to Reinforcement Learning:"},{"location":"old/policy_gradient.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"old/policy_gradient.html#policy-gradient","text":"Slides Practical session","title":"Policy Gradient"},{"location":"old/q_learning.html","text":"Introduction to Reinforcement Learning: From policy iteration to Deep Q-Learning Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"Introduction to Reinforcement Learning:"},{"location":"old/q_learning.html#introduction-to-reinforcement-learning","text":"","title":"Introduction to Reinforcement Learning:"},{"location":"old/q_learning.html#from-policy-iteration-to-deep-q-learning","text":"Slides Practical sessions: Policy iteration and Value Iteration Q-learning Deep Q-learning","title":"From policy iteration to Deep Q-Learning"},{"location":"old/text1.html","text":"Text Cleaning and Text Vectorization Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"old/text1.html#text-cleaning-and-text-vectorization","text":"Slides Practical session","title":"Text Cleaning and Text Vectorization"},{"location":"old/text2.html","text":"Words Embedding Slides Practical session","title":"Words Embedding"},{"location":"old/text2.html#words-embedding","text":"Slides Practical session","title":"Words Embedding"},{"location":"old/text3.html","text":"Text Recurrent Network Slides Practical session","title":"Text Recurrent Network"},{"location":"old/text3.html#text-recurrent-network","text":"Slides Practical session","title":"Text Recurrent Network"},{"location":"project/Assignment.html","text":"Project assignment You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"project/Assignment.html#project-assignment","text":"You now have finished the course. Congratulations! This class was split in 5 different sections. I tried to make the project assignment a combination of the different sections. You had to build a recommender system for movies first based on movie Posters and then based on movie plots and provide a webapp to use it. This project was supposed to cover the first 3 sections of the course, being the development and deployment of an AI model, recommendation systems and natural language processing. Since I saw that some of you were still struggling with the deployment part, I decided to ease the second part of the project assignment. I provided you with a pretrained reinforcement learning agent and you have to build an XAI tool to explain its decisions. For the first part of your project, you are suppose to invite me (DavidBert) to a private Github repository that contains all the code and Dockerfiles needed to run your project. You are also supposed to provide a Docker compose file that allows to run your project. I do not want to see any dataset, model weights, pickle files or annoy indexes in your repository that weights more than 100MB. Instead of that I propose that you store them on a shared Google Drive provide in your readme file a link to download them. I will manually download them and just extract the zip file in the root directory of your project. This means that if you worked correctly on the project, I should be able to run it with a single command: docker-compose up . If the project is not working, I will not try to fix it, I will just grade it as it is so make sure that it is working before submitting it ;-). Concerning the last part of the project, you are supposed to provide a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. I want to be able to run your notebook directly on Google Colab without modifying it. Finally, the project is due on February 2nd at 23:59 for INSA students and march 3rd at 23:59 for Valdom students. (INSA students were supposed to work in groups while Valdom students were supposed to work alone that is why the deadline is different). Send me an e-mail when you are done with the link to your repository to verify that everything is ok. I won't accept any project after the deadline no matter what the reason is so make sure to finish it on time. Good luck with your exams and good luck with your project!","title":"Project assignment"},{"location":"project/Docker_compose.html","text":"Docker compose example Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want. How to run Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository","title":"Help on project"},{"location":"project/Docker_compose.html#docker-compose-example","text":"Since it is not that easy to run Gradio and Annoy in a Docker container, I created an example to show how to do it to help you get started on the project. It is available here . The annoy-api and Dockefile-api files shows how to putt an Annoy index in a Docker container and expose it as a REST API. The Dockerfile-gradio and the gradio-webapp files show how to put a Gradio webapp in a Docker container. You could easily start from this example to build your recommender system. In the gradio-webapp I generate a random vector to show how to use the API. You can replace it with your own vector, computed by your model. In the annoy-api I use a fake index to show how to use the API. You can replace it with your own index, computed by your model and return as many results as you want.","title":"Docker compose example"},{"location":"project/Docker_compose.html#how-to-run","text":"Install Docker and Docker compose Clone this repository: git clone https://github.com/DavidBert/docker_compose_example.git Run docker-compose up in the root directory of this repository Open localhost:7860 in your browser Upload an image and see the results To stop the server, run docker-compose down in the root directory of this repository To remove the containers, run docker-compose rm in the root directory of this repository To remove the images, run docker image prune -a in the root directory of this repository","title":"How to run"},{"location":"rec_sys/quizz.html","text":"Quizz: Recommender systems Chargement\u2026","title":"Quizz"},{"location":"rec_sys/quizz.html#quizz-recommender-systems","text":"Chargement\u2026","title":"Quizz: Recommender systems"},{"location":"rec_sys/rec_sys.html","text":"Recommendation Systems Course: Slides recommender systems Practical session: IMDB recommender system: Solution: Project: During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. An easy way to run both the web app and the annoy index in a single container would consist to create a bash script that runs both the web app and the annoy index. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the annoy index: #!/bin/bash python python annoy_db.py & gradio_app.py The & operator is used to put jobs in the background. Here the annoy index is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the annoy index in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the annoy index in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the Annoy database. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - annoy-db annoy-db: build: context: . dockerfile: Dockerfile-annoy ports: - \"5000:5000\" Make sure in your gradion app to call the annoy database through the url http://annoy-db:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The Annoy database API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down","title":"Course and practical session"},{"location":"rec_sys/rec_sys.html#recommendation-systems","text":"","title":"Recommendation Systems"},{"location":"rec_sys/rec_sys.html#course","text":"Slides recommender systems","title":"Course:"},{"location":"rec_sys/rec_sys.html#practical-session","text":"IMDB recommender system: Solution:","title":"Practical session:"},{"location":"rec_sys/rec_sys.html#project","text":"During the practical session, you saw how to build a recommender system based on content using the movie posters. Use Gradio to build a web app that takes as input a movie poster and returns the images of the 5 most similar movies according to their poster. I would like you to mimic a real recommender system using a vector database. To do so I want the database to be requested by the web app through a REST API. The web app should be light and fast. Use a pre-trained network only to extract the vector representation of the input image and call through the REST API the annoy index you built during the practical session to find the 5 most similar movies. An easy way to run both the web app and the annoy index in a single container would consist to create a bash script that runs both the web app and the annoy index. This is not the best practice, but it might be easier for you to do so. Thus you can start with this solution and then try to run the web app and the annoy index in two different containers. Here is an example of a bash script that runs both the web app and the annoy index: #!/bin/bash python python annoy_db.py & gradio_app.py The & operator is used to put jobs in the background. Here the annoy index is run in the background and the web app is run in the foreground. Call this script in your docker file to run the application. The good practice consists in runnnig the web app in a docker container and the annoy index in another container. To do so you can use docker-compose. Look at the docker-compose documentation to learn how to use it. Here are the theroritical steps to follow to run the web app and the annoy index in two different containers using docker-compose. First, you need to create Dockerfiles for both the Gradio web app and the Annoy database. Then create a docker-compose.yml file to define and run the multi-container Docker applications. For exemple something like: version: '3.8' services: gradio-app: build: context: . dockerfile: Dockerfile-gradio ports: - \"7860:7860\" depends_on: - annoy-db annoy-db: build: context: . dockerfile: Dockerfile-annoy ports: - \"5000:5000\" Make sure in your gradion app to call the annoy database through the url http://annoy-db:5000/ as the base URL for API requests. To run the application, run the following command in the same directory as the docker-compose.yml file: docker-compose up The Gradio web app should be accessible at http://localhost:7860. The Annoy database API, if it has endpoints exposed, will be accessible at http://localhost:5000. To stop and remove the containers, networks, and volumes created by docker-compose up, run: docker-compose down","title":"Project:"},{"location":"rl/quizz.html","text":"Chargement\u2026","title":"Quizz"},{"location":"rl/rl.html","text":"Introduction to Reinforcement Learning: Slides Practical sessions: , solution: , solution: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"rl/rl.html#introduction-to-reinforcement-learning","text":"Slides Practical sessions: , solution: , solution: , solution:","title":"Introduction to Reinforcement Learning:"},{"location":"xai/interpretability.html","text":"Interpretability in Machine Learning Slides Practical session Practical session Solution","title":"Course and practical session"},{"location":"xai/interpretability.html#interpretability-in-machine-learning","text":"Slides","title":"Interpretability in Machine Learning"},{"location":"xai/interpretability.html#practical-session","text":"Practical session Solution","title":"Practical session"},{"location":"xai/project.html","text":"XAI Project. This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"xai/project.html#xai-project","text":"This is the last part of the course project. In this part, you will have to build a XAI tool for a reinforcement learning agent. You will have to use the tools seen in the course, and you can also use other tools if you want. You are supposed to return a notebook with your code and explanations. The notebook should be self-sufficient, and should be able to be run without to much effort. You will try to explain the decisions of a reinforcement learning agent in a simple video game called fruitbot from the procgen suite . You are not supposed to train the agent yourself, you can use a pretrained agent whose weights are provided here . Before starting, copy the following files ( agent.py , procgen_wrappers.py ) on your folder and install the procgen suite. pip install procgen You can instanciate the environment with the following code: import torch from procgen import ProcgenEnv from procgen_wrappers import VecExtractDictObs, TransposeFrame, ScaledFloatFrame env = ProcgenEnv( num_envs=1, env_name=\"fruitbot\", start_level=0, num_levels=100, distribution_mode='easy', ) env = VecExtractDictObs(env, \"rgb\") env = TransposeFrame(env) env = ScaledFloatFrame(env) The Agent class is defined in the agent.py file. An agent is composed of a feature extractor and a policy. The feature extractor is a neural network that takes the image as input and outputs a vector of features. The policy is a neural network that takes the features as input and outputs a score for each action. The action with the highest score is the one that is chosen. You can use it as follows: from agent import Agent agent = Agent() agent.load_state_dict(torch.load('agent_weights.pth')) agent.eval() The following code shows how to use the agent to play a game: obs = env.reset() while True: obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) if done[0]: break env.close() The goal of the project is to explain the decisions of the agent. You are exected to produce several gifs that show the agent playing the game and the explanations. Here is an example of what could be done: You can use the tools seen in the course, and you can also use other tools if you want. You are expected to produce three types of explanations. Don't be surprised if some methods don't work well on every frame (like grad-CAM), it is normal. To help you, here is a snippet that shows how to record a gif: import imageio from IPython.display import Image from tqdm.notebook import tqdm from IPython.display import clear_output import matplotlib.pyplot as plt import numpy as np def obs_to_image(obs): return (obs[0].transpose(1,2,0) * 255).astype(np.uint8) def display_trajectory(frames, fps=25): imageio.mimwrite('./tmp.gif', [obs_to_image(frame) for i, frame in enumerate(frames)], fps=fps) return(Image(open('tmp.gif','rb').read(), width=500, height=500)) frames = [] obs = env.reset() while True: frames.append(obs) obs = torch.FloatTensor(obs).to('cuda') action = agent(obs).argmax(1).cpu().numpy() obs, _, done ,_ = env.step(action) img = env.render() if done[0]: break env.close() display_trajectory(frames) You are free to code on your own machine, but you will have to submit a notebook that can be run on colab . Good luck!","title":"XAI Project."},{"location":"xai/quizz.html","text":"Chargement\u2026","title":"Quizz"}]}